{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0Y5IsnN4gkQD"},"outputs":[],"source":["# connect to the local google driver\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["## BIDIRECTIONAL ENCODER REPRESENTATIONS FROM TRANSFORMERS (BERT)"],"metadata":{"id":"_Y_GsTA_fyNp"}},{"cell_type":"markdown","source":[" ## Parameter Efficient Fine-Tuning (PEFT):\n","  A form of instruction fine-tuning that is much more efficient than full fine-tuning. Training a language model, especially for full LLM fine-tuning, demands significant computational resources. Memory allocation is not only required for storing the model but also for essential parameters during training, presenting a challenge for simple hardware. PEFT addresses this by updating only a subset of parameters, effectively “freezing” the rest. This reduces the number of trainable parameters, making memory requirements more manageable and preventing catastrophic forgetting. Unlike full fine-tuning, PEFT maintains the original LLM weights, avoiding the loss of previously learned information. This approach proves beneficial for handling storage issues when fine-tuning for multiple tasks. There are various ways of achieving Parameter efficient fine-tuning. Low-Rank Adaptation LoRA & QLoRA are the most widely used and effective.\n","\n","  ## QLoRA\n","  represents a more memory-efficient iteration of LoRA. QLoRA takes LoRA a step further by also quantizing the weights of the LoRA adapters (smaller matrices) to lower precision (e.g., 4-bit instead of 8-bit). This further reduces the memory footprint and storage requirements. In QLoRA, the pre-trained model is loaded into GPU memory with quantized 4-bit weights, in contrast to the 8-bit used in LoRA. Despite this reduction in bit precision, QLoRA maintains a comparable level of effectiveness to LoRA."],"metadata":{"id":"MwRTGmauWQfE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cb4mxam4jyCB"},"outputs":[],"source":["# install required libraries\n","# pip install -h\n","# https://pip.pypa.io/en/stable/cli/pip/ --- -q\n","# https://pip.pypa.io/en/stable/cli/pip_list/ ---- -u\n","!pip install -q -U bitsandbytes transformers datasets peft accelerate scipy einops evaluate trl rouge_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AY5AreSQPRWc"},"outputs":[],"source":["!pip install python-dotenv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Leu0tTBIjvts"},"outputs":[],"source":["import pandas as pd\n","from huggingface_hub import login, whoami\n","from dotenv import load_dotenv\n","import os\n","import torch\n","import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n","\n","from transformers import(\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorWithPadding,\n","    BitsAndBytesConfig,\n","    pipeline,\n","    set_seed,\n","    #whoami,\n",")\n","import time\n","from trl import SFTTrainer\n"]},{"cell_type":"code","source":["import transformers\n","print(transformers.__version__)"],"metadata":{"id":"zsDxHAURryI1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"97nzu4PCRZms"},"outputs":[],"source":["from datasets import Dataset"]},{"cell_type":"code","source":["import evaluate"],"metadata":{"id":"6py-QZJ0sYqj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"COUabOHS3nwL"},"outputs":[],"source":["seed = 42\n","set_seed(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A7czkzru5cbx"},"outputs":[],"source":["# # disable Weights and Biases\n","os.environ[\"WANDB_DISABLED\"] = \"true\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTzJlDmwnuCT"},"outputs":[],"source":["# login to the hugging face\n","load_dotenv(dotenv_path = \"/content/drive/MyDrive/Thesis/env/.env\")\n","api_key = os.getenv(\"HUG_FACE_API_KEY\")\n","login(api_key)\n","\n","user = whoami()\n","if user.get('emailVerified'):\n","  print(f\"{user.get('name')}...you logged successfully!!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BeMzGTJSg1Ll"},"outputs":[],"source":["# load the dataset, convert it into the hugging face dataset\n","def load_data_convert_dataset(seed = 42):\n","  # load the data\n","  train_df = pd.read_csv('/content/drive/MyDrive/Thesis/data/train/data_llm_fine_tune.csv')\n","  test_gold_df = pd.read_csv('/content/drive/MyDrive/Thesis/data/test/CT24_checkworthy_english_test_gold.tsv', delimiter = '\\t') # WO l\n","  test_df = pd.read_csv('/content/drive/MyDrive/Thesis/data/test/CT24_checkworthy_english_test.tsv', delimiter = '\\t')\n","  dev_test_df = pd.read_csv('/content/drive/MyDrive/Thesis/data/dev/CT24_checkworthy_english_dev-test.tsv', delimiter = '\\t')\n","  dev_df = pd.read_csv('/content/drive/MyDrive/Thesis/data/dev/CT24_checkworthy_english_dev.tsv', delimiter = '\\t')\n","\n","  # drop columns\n","  train_df = train_df.drop(columns = ['Unnamed: 0'])\n","  # test_gold_df = test_gold_df.drop(columns = ['Unnamed: 0'])\n","  # test_df = test_df.drop(columns = ['Unnamed: 0'])\n","\n","  # convert to Dataset\n","  train_dataset = Dataset.from_pandas(train_df)\n","  test_gold_dataset = Dataset.from_pandas(test_gold_df) # WO Label\n","  test_dataset = Dataset.from_pandas(test_df) # W L\n","  eval_test_dataset = Dataset.from_pandas(dev_test_df) # WO L\n","  eval_dataset = Dataset.from_pandas(dev_df) #  W L\n","\n","  # shuffel the dataset\n","  train_dataset = train_dataset.shuffle(seed=seed)\n","  test_gold_dataset = test_gold_dataset.shuffle(seed=seed)\n","  test_dataset = test_dataset.shuffle(seed=seed)\n","  eval_test_dataset = eval_test_dataset.shuffle(seed=seed)\n","  eval_dataset = eval_dataset.shuffle(seed=seed)\n","\n","  return train_dataset, test_gold_dataset, test_dataset,eval_test_dataset, eval_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0285TWeMg1aY"},"outputs":[],"source":["# call the function and print the features of the dataest\n","# dataset quick start : https://huggingface.co/docs/datasets/en/quickstart\n","train_dataset, test_gold_dataset, test_dataset, eval_test_dataset, eval_dataset = load_data_convert_dataset()\n","print(train_dataset.features) ## Yes/No\n","print(test_gold_dataset.features) ## Yes/No\n","print(test_dataset.features) ## Without Lable\n","print(eval_test_dataset.features) ## Yes/No\n","print(eval_dataset.features) ## Yes/No"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yMqz-g0UTpxN"},"outputs":[],"source":["eval_dataset[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWii3gH1BNf5"},"outputs":[],"source":["# 1. load dataset ---\n","# 2. create bitsandbytes configuration\n","# 3. load pretrained model\n","# 4. tokenization\n","# 5. preprocess dataset\n","# 6. prepare the model for QLoRA\n","# 7. set up PEFT for fine tuning\n","# 8. train PEFT adpater\n","# 9. evaluate the model\n","\n","##########################\n","\n","# 1. load and preprocess the dataset ----\n","# 2. tokenize the dataset\n","# 3. set up QLoRA\n","# 4. load model with QLoRA\n","# 5. Apply PEFT\n","# 6. Train PEFT Adpater\n","# 7. Evaluate the model and save"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldvgWujXWKOI"},"outputs":[],"source":["model_name = \"bert-base-uncased\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"caXFe9ZEbhNJ"},"outputs":[],"source":["# bnb config: https://huggingface.co/docs/peft/en/developer_guides/quantization\n","compute_dtype = getattr(torch, \"float16\") ##  compute_dtype = torch.float16\n","# create bitsandbytes configuration - package that provides a lightweight wrapper around custom CUDA functions that make LLMs go faster — optimizers, matrix multiplication, and quantization.\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit = True, # want to load the model in 4 bit format\n","    bnb_4bit_quant_type = \"nf4\", # 4 bit normal float\n","    bnb_4bit_use_double_quant = True,\n","    bnb_4bit_compute_dtype = compute_dtype,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ch2WUpWOdJNJ"},"outputs":[],"source":["# print(compute_dtype)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XlPxSVt2Xbp9"},"outputs":[],"source":["# https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.from_pretrained\n","# load  pretrained model  using 4-bit quantization\n","original_model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name,\n","    num_labels = 2, # binary classification\n","    quantization_config= bnb_config, # load model  using 4-bit quantization\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03fN5la8dsB5"},"outputs":[],"source":["# original_model.config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"noSVg1YEdrYz"},"outputs":[],"source":["# getting max length for embedding\n","# for generating input sequence of cinsistent length, which is beneficial for fine-tuning the language model by optimizing efficiency and minimizing computational overhead. It is essential to ensure that these sequences do not surpass the model’s maximum token limit.\n","def get_max_length(model):\n","  conf = model.config\n","  max_length = None\n","  for length_settings in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n","    max_length = getattr(conf, length_settings, None)\n","    if max_length:\n","      print(f\"Found max length: {max_length}\")\n","      break\n","  if not max_length:\n","    max_length = 512\n","    print(f\"Using declared max length: {max_length}\")\n","  return max_length"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"euyFXqrA0YZH"},"outputs":[],"source":["get_max_length(original_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TkgBloJeBNkv"},"outputs":[],"source":["# https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/bert#transformers.BertTokenizer\n","# bert is a model with absolute position embedding so it is usually advised to pad the inputs on the right  rather than the left\n","# tokenizer configuration\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","tokenizer.add_special_tokens({'pad_token':'[PAD]'})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YUc93bOfBNog"},"outputs":[],"source":["tokenizer('America stands tall again, and as a result, we are credible.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kg1EtSVJ0vln"},"outputs":[],"source":["# preprocess--- tokenize the text in batch\n","def preprocess_text_in_batch(batch, tokenizer, max_length):\n","  return tokenizer(\n","      batch[\"Text\"],\n","      max_length = max_length,\n","      truncation = True\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OMnTBBtu2H7z"},"outputs":[],"source":["# preprocess -- preprocess the dataset\n","from functools import partial\n","def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, dataset, seed):\n","  print(\"start preprocessing.......\")\n","  preprocessing_function = partial(preprocess_text_in_batch, max_length = max_length, tokenizer = tokenizer)\n","\n","  dataset = dataset.map(\n","      preprocessing_function,\n","      batched = True,\n","  )\n","  dataset = dataset.shuffle(seed = seed)\n","\n","  return dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qHkxKXBE52eq"},"outputs":[],"source":["max_length = get_max_length(original_model)\n","print(max_length)\n","train_dataset = preprocess_dataset(tokenizer, max_length, train_dataset, seed)\n","eval_dataset = preprocess_dataset(tokenizer, max_length, eval_dataset, seed)\n","test_dataset =  preprocess_dataset(tokenizer, max_length, test_dataset, seed)# WL\n","test_gold_dataset = preprocess_dataset(tokenizer, max_length, test_gold_dataset, seed) #WOL\n","eval_test_dataset = preprocess_dataset(tokenizer, max_length, eval_test_dataset, seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XRZmR4hR7UwM"},"outputs":[],"source":["print(f\"Training dataset size{train_dataset.shape}\")\n","print(f\"Validation dataset size{eval_dataset.shape}\")\n","print(f\"Test dataset size{test_gold_dataset.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A38HV84B8xvY"},"outputs":[],"source":["eval_dataset[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9g4DHJ97myu"},"outputs":[],"source":["#train_dataset[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IEggm428g1ma"},"outputs":[],"source":["# prepare model for kbit training :https://huggingface.co/docs/peft/en/developer_guides/quantization\n","# essential for QLoRA\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n","kbit_model = prepare_model_for_kbit_training(original_model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GyVMSPEbg1yO"},"outputs":[],"source":["# create LoRA config\n","# https://github.com/huggingface/peft/blob/v0.15.0/src/peft/tuners/lora/config.py#L199\n","# https://huggingface.co/docs/peft/v0.15.0/en/package_reference/lora#peft.LoraConfig\n","lora_config = LoraConfig(\n","    r = 8,\n","    lora_alpha = 16,\n","    lora_dropout=0.1,\n","    bias = \"none\",\n","    task_type=TaskType.SEQ_CLS, # text classification is kind of sequence classification, predict the next word\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvGLta7Qg14c"},"outputs":[],"source":["# enable gradient checkpointing to reduce memory usage during fine tuning\n","kbit_model.gradient_checkpointing_enable()\n","# use the get_peft_model() function to create a PEFTModel from the quantized model and configuration\n","peft_model = get_peft_model(kbit_model, lora_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jw241pWEg180"},"outputs":[],"source":["# see the trainable parameters\n","peft_model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-W4Nh2Muydcf"},"outputs":[],"source":["## rename column class_label to label\n","## https://huggingface.co/docs/datasets/v3.5.1/en/package_reference/main_classes#datasets.Dataset.rename_column\n","train_dataset = train_dataset.rename_column(\"class_label\", \"label\")\n","eval_dataset = eval_dataset.rename_column(\"class_label\", \"label\")\n","test_gold_dataset = test_gold_dataset.rename_column(\"class_label\", \"label\")\n","eval_test_dataset = eval_test_dataset.rename_column(\"class_label\", \"label\")\n","## map the label columns yes/no to 0/1\n","# https://huggingface.co/docs/datasets/en/about_map_batch\n","train_dataset = train_dataset.map(lambda x:{\"label\": 1 if x[\"label\"] == \"Yes\" else 0}) ## nedd to retirn dictionary\n","# eval_dataset = eval_dataset.map(lambda x: {1 if x[\"label\"] == \"Yes\" else 0}) ## it is returning set\n","eval_dataset = eval_dataset.map(lambda x:{\"label\": 1 if x[\"label\"] == \"Yes\" else 0})\n","test_gold_dataset = test_gold_dataset.map(lambda x:{\"label\": 1 if x[\"label\"] == \"Yes\" else 0})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UIFFSteE1EzM"},"outputs":[],"source":["# eval_dataset[0]\n","print(train_dataset.features)\n","print(eval_dataset.features)\n","print(test_gold_dataset.features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p4fn_Vt9CEA0"},"outputs":[],"source":["print(f\"Training dataset size{train_dataset.shape}\")\n","print(f\"Validation dataset size{eval_dataset.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f278bzB0g1_h"},"outputs":[],"source":["# train PEFT adapter\n","# define training arguments and create Trainer instance\n","# it has saved in gdrive... next time should be in drive ...\n","\n","def compute_metrics(eval_pred):\n","  metric = evaluate.load(\"f1\")\n","  predictions_logit, labels = eval_pred\n","  print(f\"predictions_logit: {predictions_logit}\")\n","  print(f\"labels: {labels}\")\n","  predictions = predictions_logit.argmax(axis = -1)\n","  print(f\"predictions: {predictions}\")\n","  print(f\"labels: {labels}\")\n","  # accuracy = accuracy_score(labels, predictions)\n","  # precision = precision_score(labels, predictions)\n","  # recall = recall_score(labels, predictions)\n","  # f1_score = f1_score(labels, predictions)\n","\n","  # return {\n","  #     \"acciracy\": accuracy,\n","  #     \"precision\": precision,\n","  #     \"recall\": recall,\n","  #     \"f1\": f1_score\n","  # }\n","  return {'f1': metric.compute(predictions=predictions, references=labels)}\n","\n","output_dir = f'/content/drive/MyDrive/Thesis/fine-tuning/checkworthy-binary-classification-training-{str(int(time.time()))}'\n","# Train peft Adapter- define training arguments and create trainer instance\n","args = TrainingArguments(\n","    output_dir = output_dir,\n","    # overwrite_output_dir = True,\n","    # do_eval = True,\n","    # eval_strategy = \"steps\",\n","    # gradient_accumulation_steps = 4,\n","    # max_steps = 1000,\n","    # warmup_steps = 1,\n","    # logging_steps = 25,\n","    # save_strategy = \"steps\",\n","    # save_steps = 25,\n","    # eval_steps = 25,\n","    # # for full set of optimizers: https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py\n","    # optim = \"paged_adamw_8bit\",\n","    # group_by_length = True,\n","    # report_to = None,\n","    # gradient_checkpointing = True,\n","    # # group_by_length  = True,\n","    # logging_dir = \"./logs\",\n","    # learning_rate = 2e-5,\n","\n","    #### Fact Finder #######\n","    # num_train_epochs=3,\n","    # per_device_train_batch_size=2,\n","    # gradient_accumulation_steps=2,\n","    # logging_steps=25,\n","    # optim=\"paged_adamw_32bit\",\n","    # eval_strategy=\"epoch\",\n","    # learning_rate=2e-4,\n","    # bf16=False,\n","    # fp16=False,\n","    # weight_decay=0.001,\n","    # max_grad_norm=0.3, max_steps=-1, warmup_ratio=0.03, group_by_length=True,\n","    # #run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\",\n","    # lr_scheduler_type='constant',\n","    # label_names = [\"label\"],\n","    #############################\n","    learning_rate = 2e-4,\n","    num_train_epochs = 10,\n","    logging_strategy = \"epoch\",\n","    eval_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    #load_best_model_at_end = True,\n","    #metric_for_best_model = \"f1\",\n","    label_names = [\"label\"]\n",")\n","# https://huggingface.co/docs/peft/en/task_guides/lora_based_methods\n","peft_model.config.use_cache = False\n","# The Trainer extracts labels using the label column automatically\n","peft_trainer = Trainer(\n","    model = peft_model,\n","    train_dataset = train_dataset,\n","    args = args,\n","    # it could solve the issue for compute metrics: https://discuss.huggingface.co/t/why-do-i-get-no-validation-loss-and-why-are-metrics-not-calculated/32373\n","    # https://discuss.huggingface.co/t/why-do-i-get-no-validation-loss-and-why-are-metrics-not-calculated/32373\n","    #compute_metrics = compute_metrics,\n","    data_collator = DataCollatorWithPadding(tokenizer = tokenizer,) # pad_to_multiple_of = 8)\n",")\n","# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","# https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/trainer#transformers.Trainer.data_collator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sWCWKalXg2B3"},"outputs":[],"source":["from datetime import datetime\n","start_time = datetime.now()\n","print(f\"Training started at: {start_time.strftime('%Y-%m-%d-%H-%M')}\")\n","peft_trainer.train()\n","end_time = datetime.now()\n","print(f\"Training ended at: {end_time.strftime('%Y-%m-%d-%H-%M')}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gfeKYaHUg2Ea"},"outputs":[],"source":["output_dir = \"/content/drive/MyDrive/Thesis/models/fine-tuned-bert-QLoRA\"\n","peft_trainer.save_model(output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sMAWJU4Tg2HS"},"outputs":[],"source":["# change the test dataset text to tokenized text\n","# test_gold_dataset[0] # with label\n","# test_gold_dataset = test_gold_dataset.rename_column(\"class_label\", \"label\")\n","# test_gold_dataset = test_gold_dataset.map(lambda x:{\"label\": 1 if x[\"label\"] == \"Yes\" else 0})\n","# test_dataset[0] # without label\n","# eval_test_dataset # with label\n","# eval_dataset[0] # with label\n","# predictions = peft_trainer.predict(eval_dataset)\n","#eval_prediction\n","# when i test the dataest should i remove the label columns\n","# also do in the evaluation dataset\n","# make a seperate dataset with sentence id and labels for both validation and test\n","# then compare with the prediction labels\n","# test the model\n","# Just run trainer.predict on your eval/test dataset.\n"]},{"cell_type":"code","source":["# eval_dataset[0]"],"metadata":{"id":"CmJ-0LWl7-4n"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-XopB3qg2Ju"},"outputs":[],"source":["# score on eval dataset\n","eval_prediction = peft_trainer.predict(eval_dataset)\n","logits = eval_prediction.predictions[1]\n","model_predictions_eval = logits.argmax(axis = -1)\n","print(model_predictions_eval)\n","# get the true label in array for validation dataset\n","true_labels_eval = eval_dataset['label']\n","print(true_labels_eval)\n"]},{"cell_type":"code","source":["# test data\n","# drop the label column\n","# test_prediction = peft_trainer.predict(test_gold_dataset)\n","# logits = test_prediction.predictions[1]\n","# model_predictions_test = logits.argmax(axis = -1)\n","# print(model_predictions_test)\n","\n","# true_labels_test = test_gold_dataset[\"label\"]\n","# print(true_labels_test)"],"metadata":{"id":"XQMcGezsBCe4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","f1_score_eval = f1_score(true_labels_eval, model_predictions_eval)\n","accuracy_score_eval = accuracy_score(true_labels_eval, model_predictions_eval)\n","precision_score_eval = precision_score(true_labels_eval, model_predictions_eval)\n","recall_score_eval = recall_score(true_labels_eval, model_predictions_eval)\n","\n","print(\"Scores in evaluation dataset\")\n","print(f\"Accuracy: {accuracy_score_eval}\")\n","print(f\"F1: {f1_score_eval}\")\n","print(f\"Precision: {precision_score_eval}\")\n","print(f\"Recall: {recall_score_eval}\")\n","print(\"\\n\\n\")\n","# 12:00"],"metadata":{"id":"Fg8mNWyfDjm9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # print(test_prediction.predictions[1])\n","# logits = test_prediction.predictions[1]\n","# model_predictions_test = logits.argmax(axis = -1)\n","# print(model_predictions_test)\n","\n","\n","# f1_score_test = f1_score(true_labels_test, model_predictions_test)\n","# accuracy_score_test = accuracy_score(true_labels_test, model_predictions_test)\n","# precision_score_test = precision_score(true_labels_test, model_predictions_test)\n","# recall_score_test = recall_score(true_labels_test, model_predictions_test)\n","\n","# print(\"Scores in test dataset\")\n","# print(f\"Accuracy score in test dataset: {accuracy_score_test}\")\n","# print(f\"F1 Score in test dataest: {f1_score_test}\")\n","# print(f\"Precision score in test dataset: {precision_score_test}\")\n","# print(f\"Recall score in test dataset{recall_score_test}\")"],"metadata":{"id":"-RCYp1hNCFOw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = 0\n","for i in test_prediction.predictions:\n","  print(i.shape)\n","  count += 1\n","print(count)"],"metadata":{"id":"LZn7ohsD9kcj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qR4upY6D_SjR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(np.argmax([-3.3208096,  3.2438753], axis = -1))"],"metadata":{"id":"D4VBJLmJ_e5w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# array = np.array(eval_prediction.predictions)"],"metadata":{"id":"S6WjjsW090_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prediction_array = np.array(eval_prediction.predictions)"],"metadata":{"id":"d88NNjRq9gDD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D8-g0SCig2ML"},"outputs":[],"source":["# pred_class = np.argmax(eval_prediction.predictions,axis = -1)"]},{"cell_type":"code","source":[],"metadata":{"id":"6rV6RLjt9aUm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oKZYWGUq8YJG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DlAKHiCsg2N2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1ObE3a_g2RC"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyMiU1O8RjzwWpqYVAZIraKJ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}