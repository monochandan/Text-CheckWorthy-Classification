{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPB1ujZJfLK4vsJbBGjOoML"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VoNIpqQHzy1C"},"outputs":[],"source":["from google.colab import drive\n","drive.flush_and_unmount()\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","from datetime import datetime\n","import pickle\n","import numpy as np\n","import re\n","from sklearn.decomposition import PCA\n","import joblib\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import precision_score\n","from sklearn.pipeline import Pipeline\n","# TF IDF\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import cross_val_score, train_test_split # fnding max_features\n","# from sklearn.ensemble import\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import classification_report\n","# import joblib\n","import os\n","\n","# hyperparameter tuning\n","from pprint import pprint\n","from sklearn.model_selection import RandomizedSearchCV #\n","\n","from hyperopt import fmin, tpe, hp\n","\n","import scipy.stats as stats\n","\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import  RandomForestClassifier # hp tunned\n","from sklearn.tree import DecisionTreeClassifier # hp tuned\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier #hp tuned\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from lightgbm import LGBMClassifier\n","# from sklearn.svm import SVC\n","\n","from sklearn.model_selection import GridSearchCV\n","\n","import spacy\n","import ast\n","import sklearn\n","import xgboost\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn.ensemble import VotingClassifier, StackingClassifier\n","\n","## under sampling\n","from imblearn.under_sampling import  RandomUnderSampler, EditedNearestNeighbours, CondensedNearestNeighbour, TomekLinks, ClusterCentroids, RepeatedEditedNearestNeighbours, NearMiss, NeighbourhoodCleaningRule, OneSidedSelection, AllKNN, InstanceHardnessThreshold\n","\n","## over sampling\n","from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE, KMeansSMOTE, RandomOverSampler, SMOTENC\n","\n","## over and under sampling\n","from imblearn.combine import SMOTEENN, SMOTETomek\n","\n","\n","## ctgan augmentation technique\n","# from sdv.single_table import CTGANSynthesizer\n","# from sdv.single_table import GaussianCopulaSynthesizer\n","# from sdv.single_table import TVAESynthesizer\n","# from sdv.metadata import SingleTableMetadata\n","# from sdv.evaluation.single_table import run_diagnostic\n","# from sdv.evaluation.single_table import evaluate_quality\n","\n","\n","from sklearn.cluster import MiniBatchKMeans\n","\n","\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","\n","import spacy\n","from collections import Counter\n","\n","# import re\n","\n"],"metadata":{"id":"-mr089_X0DOR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_org =  pd.read_csv('/content/drive/MyDrive/Thesis/data/train/new_11.csv')\n","df_test = pd.read_csv('/content/drive/MyDrive/Thesis/data/test/english_test_11_2.csv') # unlabeled data\n","gold_test = pd.read_csv('/content/drive/MyDrive/Thesis/data/test/CT24_checkworthy_english_test_gold.tsv',delimiter='\\t') # labeled data\n","dev_test = pd.read_csv('/content/drive/MyDrive/Thesis/data/dev/first_dev_test.csv')\n","data_to_merge = pd.read_csv('/content/drive/MyDrive/Thesis/data/train/ner_features_with_relevent_noise_prompt.csv')\n","test_tweet_f = pd.read_csv('/content/drive/MyDrive/Thesis/data/test/test_tweet_data_falcon_WL.csv')\n","test_tweet_g = pd.read_csv('/content/drive/MyDrive/Thesis/data/test/test_tweet_data_gemini_WL.csv')\n","tweet_test_wo = pd.read_csv('/content/drive/MyDrive/Thesis/data/test/tweet_test_WO_label.csv')"],"metadata":{"id":"Ux5e3bEs0lc3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorizer_path = '/content/drive/MyDrive/Thesis/models/tfidf_vectorizer_4300.pkl'\n","tfidf_vect = joblib.load(vectorizer_path)\n","\n","pca_path = '/content/drive/MyDrive/Thesis/models/pca_356.pkl'\n","pca = joblib.load(pca_path)"],"metadata":{"id":"MA731aUM1Dxm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_tweet_f.drop('Unnamed: 0', axis = 1, inplace = True)\n","test_tweet_g.drop('Unnamed: 0', axis = 1, inplace = True)"],"metadata":{"id":"MIRE-VE91h1B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test_f = test_tweet_f['label']\n","y_test_g = test_tweet_g['label']"],"metadata":{"id":"ynrKiluQ0wQv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(y_test_f.value_counts())\n","print(y_test_g.value_counts())"],"metadata":{"id":"Dxe5u29fBnF3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import string\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","stop_words = set(stopwords.words('english'))\n","\n","\n","def clean_text(text):\n","        if not isinstance(text, str):\n","            text = str(text)\n","        # remove punctuation\n","        text = ''.join([char for char in text if char not in string.punctuation]) #\n","        # tokenize\n","        tokens = word_tokenize(text)\n","        # remove stop words\n","        token = [word for word in tokens if word.lower() not in stop_words]\n","        return token\n","\n","test_tweet_f['Tokens'] = test_tweet_f['Text'].apply(clean_text)\n","test_tweet_g['Tokens'] = test_tweet_g['Text'].apply(clean_text)"],"metadata":{"id":"m18ZjxqF1QxM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test_tweet_f.head()"],"metadata":{"id":"flSDLQt03ZTd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cleaned_text(text):\n","        if not isinstance(text, str):\n","          text = str(text)\n","        # lowercase the data\n","        lower_text = text.lower()\n","        # removing the punctuations\n","        remove_punct = ''.join([char for char in lower_text if char not in string.punctuation])\n","        # remove the stop words\n","        words = word_tokenize(remove_punct)\n","        remove_stop_words = [word for word in words if word not in stop_words]\n","        # removing the numbers\n","        # removing the extra space\n","        # replace the repetations of punctuations\n","        # removing emojis\n","        # removing emoticons\n","        # removing contractions\n","\n","        return ' '.join(remove_stop_words)\n","\n","test_tweet_g['cleaned_text'] = test_tweet_g['Text'].apply(cleaned_text)\n","test_tweet_f['cleaned_text'] = test_tweet_f['Text'].apply(cleaned_text)"],"metadata":{"id":"pOw1jx7y2f01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('vader_lexicon')"],"metadata":{"id":"nv5vpqPV3m17"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.sentiment import SentimentIntensityAnalyzer\n","sia = SentimentIntensityAnalyzer()\n","\n","def get_vader_sentiment_score(text):\n","    if not isinstance(text, str):\n","      text = str(text)\n","    return sia.polarity_scores(text)\n","\n","test_tweet_f['sentiment_scores'] = test_tweet_f['Text'].apply(get_vader_sentiment_score)\n","test_tweet_g['sentiment_scores'] = test_tweet_g['Text'].apply(get_vader_sentiment_score)\n","# extract individual sentiment into seperate columns\n","test_tweet_f = pd.concat([test_tweet_f.drop(['sentiment_scores'], axis = 1),\n","                   test_tweet_f['sentiment_scores'].apply(pd.Series)], axis = 1).rename(columns= {'neg' : 'vader_neg', 'neu': 'vader_neu','pos' : 'vader_pos','compound' : 'vader_compound'})\n","\n","test_tweet_g = pd.concat([test_tweet_g.drop(['sentiment_scores'], axis = 1),\n","                   test_tweet_g['sentiment_scores'].apply(pd.Series)], axis = 1).rename(columns= {'neg' : 'vader_neg', 'neu': 'vader_neu','pos' : 'vader_pos','compound' : 'vader_compound'})"],"metadata":{"id":"PXjNnYj44dhX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test_tweet_f.head()"],"metadata":{"id":"iuKF8w2-4_jW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")"],"metadata":{"id":"fE2Gl0Uc5dHL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_sentiment_features(text):\n","    if not isinstance(text, str):\n","        text = str(text)\n","    # Tokenize the input text\n","    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n","\n","    # Perform inference\n","    outputs = model(**tokens)\n","    logits = outputs.logits\n","\n","    # Apply softmax to calculate probabilities\n","    probs = torch.nn.functional.softmax(logits, dim=-1)\n","    negative, neutral, positive = probs[0].tolist()\n","\n","    # Define thresholds for \"mixed\" classification (adjust as needed)\n","    if max([positive, negative, neutral]) == positive and positive > 0.5:\n","        sentiment = \"Positive\"\n","    elif max([positive, negative, neutral]) == negative and negative > 0.5:\n","        sentiment = \"Negative\"\n","    else:\n","        sentiment = \"Mixed\"\n","\n","    return {\n","        \"positive_score\": positive,\n","        \"negative_score\": negative,\n","        \"neutral_score\": neutral,\n","        \"sentiment_class\": sentiment\n","    }"],"metadata":{"id":"QNchVlld5-p_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_tweet_f['roberta_sentiment'] = test_tweet_f['Text'].apply(get_sentiment_features)\n","test_tweet_g['roberta_sentiment'] = test_tweet_g['Text'].apply(get_sentiment_features)"],"metadata":{"id":"j0NzLMEJ6PZy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_tweet_f[\"positive_score\"] = test_tweet_f[\"roberta_sentiment\"].apply(lambda x: x[\"positive_score\"])\n","test_tweet_f[\"negative_score\"] = test_tweet_f[\"roberta_sentiment\"].apply(lambda x: x[\"negative_score\"])\n","test_tweet_f[\"neutral_score\"] = test_tweet_f[\"roberta_sentiment\"].apply(lambda x: x[\"neutral_score\"])\n","test_tweet_f[\"sentiment_class\"] = test_tweet_f[\"roberta_sentiment\"].apply(lambda x: x[\"sentiment_class\"])\n","\n","test_tweet_f[\"is_positive\"] = test_tweet_f[\"sentiment_class\"].apply(lambda x: 1 if x == \"Positive\" else 0)\n","test_tweet_f[\"is_negative\"] = test_tweet_f[\"sentiment_class\"].apply(lambda x: 1 if x == \"Negative\" else 0)\n","test_tweet_f[\"is_mixed\"] = test_tweet_f[\"sentiment_class\"].apply(lambda x: 1 if x == \"Mixed\" else 0)\n","\n","\n","test_tweet_g[\"positive_score\"] = test_tweet_g[\"roberta_sentiment\"].apply(lambda x: x[\"positive_score\"])\n","test_tweet_g[\"negative_score\"] = test_tweet_g[\"roberta_sentiment\"].apply(lambda x: x[\"negative_score\"])\n","test_tweet_g[\"neutral_score\"] = test_tweet_g[\"roberta_sentiment\"].apply(lambda x: x[\"neutral_score\"])\n","test_tweet_g[\"sentiment_class\"] = test_tweet_g[\"roberta_sentiment\"].apply(lambda x: x[\"sentiment_class\"])\n","\n","test_tweet_g[\"is_positive\"] = test_tweet_g[\"sentiment_class\"].apply(lambda x: 1 if x == \"Positive\" else 0)\n","test_tweet_g[\"is_negative\"] = test_tweet_g[\"sentiment_class\"].apply(lambda x: 1 if x == \"Negative\" else 0)\n","test_tweet_g[\"is_mixed\"] = test_tweet_g[\"sentiment_class\"].apply(lambda x: 1 if x == \"Mixed\" else 0)"],"metadata":{"id":"KjJJDmP36VXy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_tweet_g.head()"],"metadata":{"id":"uARWhKak7D8s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_tokens(tokens):\n","    return len(tokens)\n","test_tweet_f['token_length'] = test_tweet_f['Tokens'].apply(count_tokens)\n","test_tweet_g['token_length'] = test_tweet_g['Tokens'].apply(count_tokens)"],"metadata":{"id":"Z7vvEuxr7GXU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"id":"_aByAv5R7VcA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_entities_to_columns(text):\n","\n","\n","    # # Process each row in the DataFrame\n","    # for text in df[text_column]:\n","    #     # Default counts for each row\n","    names = 0\n","    orgs = 0\n","    dates = 0\n","\n","    if not isinstance(text, str):\n","        text = str(text)\n","\n","    # Process text using spaCy\n","    doc = nlp(text)\n","    for ent in doc.ents:\n","        if ent.label_ == \"PERSON\":\n","            names += 1\n","        elif ent.label_ == \"ORG\":\n","            orgs += 1\n","        elif ent.label_ == \"DATE\":\n","            dates += 1\n","\n","    return {\n","        \"names\": names,\n","        \"organizations\": orgs,\n","        \"dates\": dates\n","    }"],"metadata":{"id":"EKE-L6Fw7XeO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_tweet_g['counts'] = test_tweet_g['Text'].apply(count_entities_to_columns)\n","test_tweet_f['counts'] = test_tweet_f['Text'].apply(count_entities_to_columns)"],"metadata":{"id":"YvJJ0-LM7eIb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_tweet_g[\"names\"] = test_tweet_g[\"counts\"].apply(lambda x: x[\"names\"])\n","test_tweet_g[\"organizations\"] = test_tweet_g[\"counts\"].apply(lambda x: x[\"organizations\"])\n","test_tweet_g[\"dates\"] = test_tweet_g[\"counts\"].apply(lambda x: x[\"dates\"])\n","\n","test_tweet_f[\"names\"] = test_tweet_f[\"counts\"].apply(lambda x: x[\"names\"])\n","test_tweet_f[\"organizations\"] = test_tweet_f[\"counts\"].apply(lambda x: x[\"organizations\"])\n","test_tweet_f[\"dates\"] = test_tweet_f[\"counts\"].apply(lambda x: x[\"dates\"])"],"metadata":{"id":"DZk3i_ud7pT0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_punctuation(text):\n","    if not isinstance(text, str):\n","        text = str(text)\n","    # Define a regular expression pattern to match punctuation characters\n","    punctuation_pattern = r'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'\n","    # Find all punctuation marks in the text\n","    punctuation_count = len(re.findall(punctuation_pattern, text))\n","    return punctuation_count"],"metadata":{"id":"d8eGg09X74P7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_tweet_f['punctuation_count'] = test_tweet_f['Text'].apply(count_punctuation)\n","test_tweet_g['punctuation_count'] = test_tweet_g['Text'].apply(count_punctuation)"],"metadata":{"id":"uhrl5dnw78x2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_tweet_f.columns"],"metadata":{"id":"_ApCDoMT7_Hv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def text_length(text):\n","  if not isinstance(text, str):\n","    text = str(text)\n","  return len(text)"],"metadata":{"id":"sQXNFgB38NBF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_tweet_f['text_length'] = test_tweet_f['Text'].apply(text_length)\n","test_tweet_g['text_length'] = test_tweet_g['Text'].apply(text_length)\n","\n","test_tweet_f['cleaned_text_length'] = test_tweet_f['cleaned_text'].apply(text_length)\n","test_tweet_g['cleaned_text_length'] = test_tweet_g['cleaned_text'].apply(text_length)"],"metadata":{"id":"a0W7VsabAYkr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_tweet_f.columns"],"metadata":{"id":"myI2rSBAAhIo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_tweet_f.rename(columns = {'raw_text_length': 'text_length', 'negative_score': 'roberta_sent_neg', 'neutral_score': 'roberta_sent_mixed', 'positive_score': 'roberta_sent_pos', 'token_length': 'count_tokens'}, inplace = True)\n","\n","test_tweet_g.rename(columns = {'raw_text_length': 'text_length', 'negative_score': 'roberta_sent_neg', 'neutral_score': 'roberta_sent_mixed', 'positive_score': 'roberta_sent_pos', 'token_length': 'count_tokens'}, inplace = True)"],"metadata":{"id":"Fk2vKTWXA0iu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_tweet_g.to_csv('/content/drive/MyDrive/Thesis/data/test/test_tweet_g.csv', index = False)\n","test_tweet_f.to_csv('/content/drive/MyDrive/Thesis/data/test/test_tweet_f.csv', index = False)"],"metadata":{"id":"a5_8EoU5A_xv"},"execution_count":null,"outputs":[]}]}