{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRTg9Mjd92gJ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6vswsApMBe0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# from sklearn.decomposition import PCA\n",
    "# import joblib\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import recall_score\n",
    "# from sklearn.metrics import precision_score\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# # TF IDF\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import cross_val_score, train_test_split # fnding max_features\n",
    "# # from sklearn.ensemble import\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.metrics import classification_report\n",
    "# # import joblib\n",
    "# import os\n",
    "\n",
    "# # hyperparameter tuning\n",
    "# from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier # to look availiable hyperparameters\n",
    "# from pprint import pprint\n",
    "# from sklearn.model_selection import RandomizedSearchCV #\n",
    "\n",
    "# from hyperopt import fmin, tpe, hp\n",
    "\n",
    "# import scipy.stats as stats\n",
    "# import xgboost as xgb\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# import spacy\n",
    "# import ast\n",
    "# import sklearn\n",
    "# import xgboost\n",
    "\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyqbGxB7MNmb"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/content/drive/MyDrive/Thesis/data/train/ner_features.csv')\n",
    "test_tweet_data = pd.read_csv('/content/drive/MyDrive/Thesis/data/test/final_tweet_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSBnymsUnGtJ"
   },
   "outputs": [],
   "source": [
    "# tweet data lebeld with falcon llm and gemini llm\n",
    "tweet_data_f = pd.read_csv('/content/drive/MyDrive/Thesis/data/test/gold_tweet_test_falcon_7b.csv')\n",
    "tweet_data_g = pd.read_csv('/content/drive/MyDrive/Thesis/data/test/gold_tweet_test_gemini.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNWzb-g6nGcw"
   },
   "outputs": [],
   "source": [
    "print(tweet_data_f['label'].value_counts()) # falcon\n",
    "print(tweet_data_g['label'].value_counts()) # gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeS2nuMJ70O9"
   },
   "outputs": [],
   "source": [
    "print(tweet_data_f.shape)\n",
    "print(tweet_data_g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0fi-EspaiUt"
   },
   "outputs": [],
   "source": [
    "tweet_data_g.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jktwv9r0aszA"
   },
   "outputs": [],
   "source": [
    "# modify some errors - drop columns, vaue changing\n",
    "# tweet_data_f = tweet_data_f.drop('Unnamed: 0', axis = 1)\n",
    "# tweet_data_g = tweet_data_g.drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "tweet_data_g['label'] = tweet_data_g['label'].apply(lambda x: re.sub('\\n', ' ', x).strip())\n",
    "tweet_data_g['label'] = tweet_data_g['label'].apply(lambda x: 1 if x == \"Yes\" else 0 )\n",
    "tweet_data_g.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1G-mQ1X06gv"
   },
   "outputs": [],
   "source": [
    "# save as final dataset, which will be use for testing the model\n",
    "tweet_data_f.to_csv('/content/drive/MyDrive/Thesis/data/test/test_tweet_data_falcon_WL.csv', index = False)\n",
    "tweet_data_g.to_csv('/content/drive/MyDrive/Thesis/data/test/test_tweet_data_gemini_WL.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5qGq6uZMZEt"
   },
   "outputs": [],
   "source": [
    "test_tweet_data.rename(columns={'after_removing_nl;;;' : 'Text'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmbH85NYar8o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TQmcZGpCz3t"
   },
   "outputs": [],
   "source": [
    "test_tweet_data.iloc[1].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HB9POEpHDGnp"
   },
   "outputs": [],
   "source": [
    "def column_readjusting(df):\n",
    "  i = 1\n",
    "  for index, row in test_tweet_data.iterrows():\n",
    "    if pd.isna(row['Text']):\n",
    "      text = str(row['tweet_id'])\n",
    "      clean_text = re.sub(r'^(\\d+),', '', text).strip()\n",
    "      df.at[index, 'Text'] = clean_text\n",
    "\n",
    "    df.at[index, 'tweet_id'] = int(i)\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXUOUl0cDGp7"
   },
   "outputs": [],
   "source": [
    "column_readjusting(test_tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AqV8FriHnMuB"
   },
   "outputs": [],
   "source": [
    "test_tweet_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9gutaAgDGr5"
   },
   "outputs": [],
   "source": [
    "test_tweet_data.to_csv('/content/drive/MyDrive/Thesis/data/test/first_tweet_datas_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UamBseBBDGtq"
   },
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('/content/drive/MyDrive/Thesis/data/test/first_tweet_datas_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6xDDwNODGv_"
   },
   "outputs": [],
   "source": [
    "new_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCkvDmlBDGx6"
   },
   "outputs": [],
   "source": [
    "test_tweet_data['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwPacaFTDG0U"
   },
   "outputs": [],
   "source": [
    "test_tweet_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "falDBMqRDG3N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULKSnMHRDG4w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJyAso2NDG7Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENA5TA78DG9d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05DKNNnHDHBX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TSOjvE4DHC_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWACPDHEDHFN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWbwcUloDHH_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBlyunFKfNu0"
   },
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oS9RP79rMbiS"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2ofVISUWKi1"
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "model_name =  \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "## to reducing the memory usages:\n",
    "## https://medium.com/%40rakeshrajpurohit/model-quantization-with-hugging-face-transformers-and-bitsandbytes-integration-b4c9983e8996\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_name,\n",
    "          quantization_config=quantization_config,\n",
    "          #load_in_4bit=True,\n",
    "          device_map = \"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model = model,\n",
    "#     tokenizer = tokenizer,\n",
    "#     torch_dtype = torch.bfloat16,\n",
    "#     device_map = \"auto\",\n",
    "#     # max_new_tokens = 20,\n",
    "#     # device=0 if torch.cuda.is_available() else -1,\n",
    "# )\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07LFciO8indU"
   },
   "outputs": [],
   "source": [
    "# 26 11:00\n",
    "# Check for GPU in Colab\n",
    "# import torch\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"CUDA is available. GPU is detected.\")\n",
    "# else:\n",
    "#     print(\"CUDA is not available. Please ensure you are using a GPU runtime in Colab.\")\n",
    "#     print(\"Go to Runtime -> Change runtime type -> Select GPU as Hardware accelerator\")\n",
    "\n",
    "# Install multi-platform bitsandbytes for Colab\n",
    "# !pip install bitsandbytes-cuda120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0piV4VINQAsH"
   },
   "source": [
    "- create function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAnfkKiFTeEm"
   },
   "outputs": [],
   "source": [
    "def ad_prompt_column(row):\n",
    "    # action_type = ['Physical_Action', 'Mental_Action', 'State_Change', 'Create_or_Destruction', 'Communication', 'Movement', 'Emotion_or_Feeling' , 'Perception', 'Linking(State_of_being)Verb', 'Other']\n",
    "    # noise_level = ['Relevant', 'Noisy']\n",
    "    # Intent = ['Statement_of_Fact', 'Opinion_or_Belief', 'Argument_or_Justification', 'Action_or_Instruction', 'No _Clear_Intent']\n",
    "\n",
    "    # sentiment = ['Positive', 'Negative', 'Other(Unclear/Noisy)']\n",
    "    text = row['Text']\n",
    "    # prompt_1 = f\"Is the following text '{text}' Relevant or Noise. Tell me in one word\"\n",
    "\n",
    "    # prompt_2 = f\"\"\"Classify this {text} as 'Yes' (check-worthy) or 'No' (not check-worthy) based on:\n",
    "    #           - If it contains a **verifiable factual claim**.\n",
    "    #           - If it is **harmful** or **misleading**\"\"\"\n",
    "\n",
    "    # this prompt used for classify the scrapped tweet text, to add their labels\n",
    "    prompt_3 = (\n",
    "    \"Read the following statement carefully. \"\n",
    "    \"Your task is to determine whether the statement contains claims or information that require fact-checking. \"\n",
    "    \"- If the statement presents factual assertions, statistics, or claims that can be verified for accuracy, respond with 'Yes'. \"\n",
    "    \"- If the statement is purely opinion-based, general knowledge, trivial, or lacks verifiable claims, respond with 'No'.\\n\\n\"\n",
    "    f\"Statement: '{text}'\\nResponse:\"\n",
    "    )\n",
    "\n",
    "    # prompt_for_finding_action_verb = f\"Is the following text '{text}' contains any action verb, Yes or No. Tell me in one word\"\n",
    "\n",
    "    # prompt_for_catagory_of_sentence = f\"Can you catagorize this following text '{text}' into one of this categories: Social, Financial, Governmental, Political, Commercial, Constitutional and Environmental\"\n",
    "    # f\"\\n1. Relevant\"\n",
    "    # f\"\\n2. Noisy\"\n",
    "    # f\"\\n Generate Only the choosen option: \"\n",
    "\n",
    "            #         Intent (Choose One):\n",
    "            #         - Statement of Fact\n",
    "            #         - Opinion or Belief\n",
    "            #         - Argument or Justification\n",
    "            #         - Action or Instruction\n",
    "\n",
    "            #         Sentiment (Choose One):\n",
    "            #         - Positive\n",
    "            #         - Negative\n",
    "            #         - Neutral\n",
    "            #         - Other (Unclear/Noisy)\n",
    "\n",
    "            #         Provide the response in a structured format like:\n",
    "            #         Action Type: [Category]\n",
    "            #         Noise Level: [Category]\n",
    "            #         Intent: [Category]\n",
    "            #         Sentiment: [Category]\n",
    "            # \"\"\"\n",
    "    #response =\n",
    "    #print(response)\n",
    "  ############################# generating lables for tweet data ()####################################################\n",
    "    inputs = tokenizer(prompt_3, return_tensors = \"pt\").to(model.device)\n",
    "    # print(inputs)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 100, do_sample = False)\n",
    "    # print('+'*30)\n",
    "    # print('OUTPUT')\n",
    "    # print(outputs)\n",
    "\n",
    "    generate_result = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "    # print(\"generate_result\")\n",
    "    # print(generate_result)\n",
    "\n",
    "    print(\"+\"*30)\n",
    "\n",
    "    response = re.sub('\\n', ' ', generate_result).strip().split(' ')[-1]\n",
    "\n",
    "    if response == \"'Yes'\":\n",
    "      row['label'] = 1\n",
    "    else:\n",
    "      row['label'] = 0\n",
    "  #####################################################################################\n",
    "    # inputs = tokenizer(prompt, return_tensors = \"pt\").to(model.device)\n",
    "    # # print(inputs)\n",
    "\n",
    "    # outputs = model.generate(**inputs, max_new_tokens = 100, do_sample = False)\n",
    "    # print('+'*30)\n",
    "    # print('OUTPUT')\n",
    "    # print(outputs)\n",
    "\n",
    "    # generate_result = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "    # print(\"generate_result\")\n",
    "    # print(generate_result)\n",
    "\n",
    "    # print(\"+\"*30)\n",
    "\n",
    "    # row['Noise_text'] = False\n",
    "    # row['Relevant_text'] = False\n",
    "\n",
    "    # if 'Noise' in generate_result.split():\n",
    "    #     row['Noise_text'] = True\n",
    "    # elif 'Relevant' in generate_result.split():\n",
    "    #     row['Relevant_text'] = True\n",
    "    # return row\n",
    "  ##############################################################################################\n",
    "    # start_indx = generate_result.find(\"Action Type: \")\n",
    "\n",
    "    # print(generate_result[start_indx:])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PS_YPrTbo7Pk"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  test_tweet_data = test_tweet_data.apply(ad_prompt_column, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxpPtZf9wg3c"
   },
   "outputs": [],
   "source": [
    "test_tweet_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1PzAKmC1q5N"
   },
   "outputs": [],
   "source": [
    "# save this data with lebel and without lebel\n",
    "test_tweet_data.to_csv('/content/drive/MyDrive/Thesis/data/test/gold_tweet_test_falcon_7b.csv')\n",
    "test_tweet_data.drop(['label'], axis = 1, inplace = True)\n",
    "test_tweet_data.to_csv('/content/drive/MyDrive/Thesis/data/test/tweet_test_falcon_7b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sv0To_1Po47e"
   },
   "outputs": [],
   "source": [
    "## using batch size to faster or efficient run\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ZlYHXP2H6_5a"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  train_df = train_df.apply(ad_prompt_column, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huGxrPZYc7yW"
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('/content/drive/MyDrive/Thesis/data/train/ner_features_with_relevent_noise_prompt.csv', index = False)\n",
    "# batch_size = 20\n",
    "# for i in range(0, len(demo_train_df), batch_size):\n",
    "#   batch_text = demo_train_df['Text'][i:i+batch_size].tolist()\n",
    "#   print(batch_text)\n",
    "#   results = ner_text(batch_text)\n",
    "#   print(results)\n",
    "#   demo_train_df.loc[i:i+batch_size-1, 'ner_result'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMIDQCcg94Ek"
   },
   "outputs": [],
   "source": [
    "def see_the_columns_values(id, df):\n",
    "    print(f\"Original text: {df[df['Sentence_id'] == id]['Text'].tolist()}\")\n",
    "    print(f\"NER: {df[df['Sentence_id'] == id]['ner_result'].tolist()}\")\n",
    "    # print(f\"names: {df[df['Sentence_id'] == id]['names'].tolist()}\")\n",
    "    # print(f\"organizations: {df[df['Sentence_id'] == id]['organizations'].tolist()}\")\n",
    "    # print(f\"locations: {new_df[new_df['Sentence_id'] == id]['locations'].tolist()}\")\n",
    "    # print(f\"dates: {new_df[new_df['Sentence_id'] == id]['dates'].tolist()}\")\n",
    "    # print(f\"verbs: {new_df[new_df['Sentence_id'] == id]['verbs'].tolist()}\")\n",
    "    # print(f\"lemmatized verbs: {new_df[new_df['Sentence_id'] == id]['lemmatized_verb'].tolist()}\")\n",
    "    # print(f\"action verbs: {new_df[new_df['Sentence_id'] == id]['action_verbs'].tolist()}\")\n",
    "    # print(f\"filtered action verbs: {new_df[new_df['Sentence_id'] == id]['filtered_action_verbs'].tolist()}\")\n",
    "    # print(f\"joined tokens: {new_df[new_df['Sentence_id'] == id]['joined_tokens'].tolist()}\")\n",
    "    # print(f\"count verbs: {new_df[new_df['Sentence_id'] == id]['count_verb'].tolist()}\")\n",
    "    # print(f\"count action verbs: {new_df[new_df['Sentence_id'] == id]['count_action_verb'].tolist()}\")\n",
    "    #  ###### 'names', 'organizations', 'locations','dates'\n",
    "    # print(f\"count action verbs: {new_df[new_df['Sentence_id'] == id]['names'].tolist()}\")\n",
    "    # print(f\"count action verbs: {new_df[new_df['Sentence_id'] == id]['organizations'].tolist()}\")\n",
    "    # print(f\"count action verbs: {new_df[new_df['Sentence_id'] == id]['locations'].tolist()}\")\n",
    "    # print(f\"count action verbs: {new_df[new_df['Sentence_id'] == id]['dates'].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPvbK-qm_FIw"
   },
   "outputs": [],
   "source": [
    "see_the_columns_values(22058, demo_train_df)\n",
    "# demo_train_df[demo_train_df['class_label'] == 'Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ATV6ubbgSoK"
   },
   "outputs": [],
   "source": [
    "train_df['informative_text_res'] = train_df['Text'].apply(classify_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnDxASJBc8Br"
   },
   "outputs": [],
   "source": [
    "train_df.drop(['check_ner', 'informative_text_prompt'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zu_ymU_oWBkr"
   },
   "outputs": [],
   "source": [
    "# train_df['informative_text_prompt'] = train_df['Text'].apply(classify_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTtZfcKqU7zV"
   },
   "outputs": [],
   "source": [
    "# sequences = pipe(\n",
    "#     prompt,\n",
    "#     max_new_tokens = 100,\n",
    "#     return_full_text = False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0Ow2c_YX8wK"
   },
   "outputs": [],
   "source": [
    "# for seq in sequences:\n",
    "#   print(f\"Result: {seq['generated_text']}\")\n",
    "x = sequences[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5KiexqeYPvF"
   },
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ats_99tb8ii"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_05PWnKq5xbs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-ncjqvtBB6p"
   },
   "outputs": [],
   "source": [
    "text = \"If we're $4 trillion down, we should have everything perfect, but we don't.\"\n",
    "res = fun(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFGVtlyxBI3D"
   },
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zwuh80zCBw6a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPp3enKUV90XrACSh6Z9ePO",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
