{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0839cd-bd0b-48fd-b9d7-59f8db5ad9a7",
   "metadata": {},
   "source": [
    "### test the model on given english test data and then compare with test gold data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0eaa43-7c76-4eab-8d6c-966ca7bc4bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "# import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "# import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca0de51-86d9-4bb1-81f7-73f93f046507",
   "metadata": {},
   "source": [
    "# 1. load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08f5c14-3561-47bd-bca5-dbc4e96b1867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to test_english\n",
    "df = pd.read_csv('data/CT24_checkworthy_english/CT24_checkworthy_english/tset_english/CT24_checkworthy_english_test.tsv', delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71fd900c-553a-44c6-b3ad-36539153ea4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35988</td>\n",
       "      <td>They said they were just going to get inspecto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35991</td>\n",
       "      <td>And from that point on, I've voted to -- I mov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36029</td>\n",
       "      <td>I sit on the Senate Armed Services Committee.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36043</td>\n",
       "      <td>We need to depend on all of our tools -- diplo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36063</td>\n",
       "      <td>And that is -- and I don't know if my colleagu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence_id                                               Text\n",
       "0        35988  They said they were just going to get inspecto...\n",
       "1        35991  And from that point on, I've voted to -- I mov...\n",
       "2        36029      I sit on the Senate Armed Services Committee.\n",
       "3        36043  We need to depend on all of our tools -- diplo...\n",
       "4        36063  And that is -- and I don't know if my colleagu..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.drop('raw_text_length', axis = 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b239b1-4d56-4844-8229-182761d5acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new label from text to 0/1\n",
    "# df['label'] = df['class_label'].apply(lambda x: 1 if x == 'Yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d312b8c3-8d02-44d2-93ae-fd9a9fdc18c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\looka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\looka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d99f63-65d3-4418-8844-604e1b42978a",
   "metadata": {},
   "source": [
    "# 2. generates tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5fc1dd7-ad65-40cc-b6ca-2f4ef8f3b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation]) # \n",
    "    # tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # remove stop words\n",
    "    token = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9ede96e-96e3-4ead-a5cb-732f3714e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new column name Token\n",
    "df['Tokens'] = df['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879feeb0-af07-4353-80c8-9a9604637237",
   "metadata": {},
   "source": [
    "# 3. row text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "650eeb29-73bb-43c3-b5d8-125b252c52af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verious_text_length(text):\n",
    "    text_length = len(text)\n",
    "    return text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92dcbf0f-eb50-4761-9ad1-d3aec2f40cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_length'] = df['Text'].apply(verious_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbbada04-88b3-4869-8b2b-ac942f07dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_text(text):\n",
    "    # lowercase the data\n",
    "    lower_text = text.lower()\n",
    "    # removing the punctuations\n",
    "    remove_punct = ''.join([char for char in lower_text if char not in string.punctuation])\n",
    "    # remove the stop words\n",
    "    words = word_tokenize(remove_punct)\n",
    "    remove_stop_words = [word for word in words if word not in stop_words]\n",
    "    # removing the numbers\n",
    "    # removing the extra space\n",
    "    # replace the repetations of punctuations\n",
    "    # removing emojis\n",
    "    # removing emoticons\n",
    "    # removing contractions\n",
    "    \n",
    "    return ' '.join(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68f93f-372e-4091-8f94-c8e333fcc0f4",
   "metadata": {},
   "source": [
    "# 4. clean text and cleaned text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66729d4d-979a-4e84-bbe3-844028d14ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['Text'].apply(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "809e5443-b004-4ac4-aa88-283a42b74b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text_length'] = df['cleaned_text'].apply(verious_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aca796d1-7da3-469b-8399-11f1a71eb879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c8197b7-bafe-41be-96d3-261e75bfcdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "334eeb94-ac00-4ea8-8a0d-e0aed529bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_vader_sentiment_score(text):\n",
    "#     return sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "376ac0c2-ed84-4a72-8a8d-0545ccfe1875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new['sentiment_scores'] = df_new['Text'].apply(get_vader_sentiment_score) \n",
    "# # extract individual sentiment into seperate columns\n",
    "# df_new = pd.concat([df_new.drop(['sentiment_scores'], axis = 1), \n",
    "#                    df_new['sentiment_scores'].apply(pd.Series)], axis = 1).rename(columns= {'neg' : 'vader_neg', 'neu': 'vader_neu','pos' : 'vader_pos','compound' : 'vader_compound'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c15328e1-f1c9-4b74-988d-57a9b274e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c47760a-004a-4be4-af43-fa8fd84080c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_features(text):\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Perform inference\n",
    "    outputs = model(**tokens)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    negative, neutral, positive = probs[0].tolist()\n",
    "    \n",
    "    # Define thresholds for \"mixed\" classification (adjust as needed)\n",
    "    if max([positive, negative, neutral]) == positive and positive > 0.5:\n",
    "        sentiment = \"Positive\"\n",
    "    elif max([positive, negative, neutral]) == negative and negative > 0.5:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Mixed\"\n",
    "    \n",
    "    return {\n",
    "        \"positive_score\": positive,\n",
    "        \"negative_score\": negative,\n",
    "        \"neutral_score\": neutral,\n",
    "        \"sentiment_class\": sentiment\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4badd003-1ff7-4c55-bbc2-e0805652c0f7",
   "metadata": {},
   "source": [
    "# 5. roberta sentiment generate, then different sentiment put in seperate coulumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6094397-f8c5-4d39-afc4-a75a7ac1d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['roberta_sentiment'] = df['Text'].apply(get_sentiment_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "155a5bfe-4b74-496d-ac63-10aa51ef3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"roberta_sent_pos\"] = df[\"roberta_sentiment\"].apply(lambda x: x[\"positive_score\"])\n",
    "df[\"roberta_sent_neg\"] = df[\"roberta_sentiment\"].apply(lambda x: x[\"negative_score\"])\n",
    "df[\"roberta_sent_mixed\"] = df[\"roberta_sentiment\"].apply(lambda x: x[\"neutral_score\"])\n",
    "df[\"roberta_sentiment_class\"] = df[\"roberta_sentiment\"].apply(lambda x: x[\"sentiment_class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a113c4c-e8a5-474a-bc3c-0b243fb785c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new[\"is_positive\"] = df_new[\"sentiment_class\"].apply(lambda x: 1 if x == \"Positive\" else 0)\n",
    "# df_new[\"is_negative\"] = df_new[\"sentiment_class\"].apply(lambda x: 1 if x == \"Negative\" else 0)\n",
    "# df_new[\"is_mixed\"] = df_new[\"sentiment_class\"].apply(lambda x: 1 if x == \"Mixed\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8af1eb8-9629-45b4-945e-3663816bfbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(tokens):\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27539f4a-3da9-45a2-9041-afd0dffee5e2",
   "metadata": {},
   "source": [
    "# 6. token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9daff467-da52-4ca6-b92b-3c69d1668caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token_length'] = df['Tokens'].apply(count_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f77f3baf-6ccb-481e-8561-54d982775346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\looka\\OneDrive\\Documents\\Thesis\\master_thesis\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cbcc8b-9ef7-4a51-9f66-ad58349b7e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def entity_counts(text):\n",
    "#     entities = ner_pipeline(text)\n",
    "    \n",
    "#     # Initialize counts for each entity type\n",
    "#     name_count = 0\n",
    "#     organization_count = 0\n",
    "#     date_count = 0\n",
    "    \n",
    "#     # Count entities based on their type\n",
    "#     for entity in entities:\n",
    "#         if entity[\"entity_group\"] == \"PER\":  # Person\n",
    "#             name_count += 1\n",
    "#         elif entity[\"entity_group\"] == \"ORG\":  # Organization\n",
    "#             organization_count += 1\n",
    "#         elif entity[\"entity_group\"] == \"DATE\":  # Date\n",
    "#             date_count += 1\n",
    "    \n",
    "#     # Return the counts as a pandas Series\n",
    "#     return pd.Series([name_count, organization_count, date_count])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584e1f6-5c0b-4e58-804f-fefe050acd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new[[a'name_count', 'organization_count', 'date_count']] = df_new['Text'].apply(entity_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44910087-01db-4391-adb7-bcfaa60a1bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4944be47-3d96-4313-9996-415be2f701da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entities_to_columns(text):\n",
    "    \n",
    "\n",
    "    # # Process each row in the DataFrame\n",
    "    # for text in df[text_column]:\n",
    "    #     # Default counts for each row\n",
    "    names = 0\n",
    "    orgs = 0\n",
    "    dates = 0\n",
    "\n",
    "        # Process text using spaCy\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            names += 1\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            orgs += 1\n",
    "        elif ent.label_ == \"DATE\":\n",
    "            dates += 1\n",
    "\n",
    "    return {\n",
    "        \"names\": names,\n",
    "        \"organizations\": orgs,\n",
    "        \"dates\": dates\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dfd5fb-f1a9-44ae-b291-8694b2936db2",
   "metadata": {},
   "source": [
    "# 7. count name organization and date, then out in the seperate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77e924ab-036a-4c03-9544-ef482e8357b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['counts'] = df['Text'].apply(count_entities_to_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edaeee29-6862-403b-8615-e599da72eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"names\"] = df[\"counts\"].apply(lambda x: x[\"names\"])\n",
    "df[\"organizations\"] = df[\"counts\"].apply(lambda x: x[\"organizations\"])\n",
    "df[\"dates\"] = df[\"counts\"].apply(lambda x: x[\"dates\"])\n",
    "# df_new[\"sentiment_class\"] = df_new[\"roberta_sentiment\"].apply(lambda x: x[\"sentiment_class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d3d3da0-98f4-43e5-885e-2bf831c7897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_punctuation(text):\n",
    "    # Define a regular expression pattern to match punctuation characters\n",
    "    punctuation_pattern = r'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'\n",
    "    # Find all punctuation marks in the text\n",
    "    punctuation_count = len(re.findall(punctuation_pattern, text))\n",
    "    return punctuation_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9ad18f-3307-428a-95a1-2973395874d2",
   "metadata": {},
   "source": [
    "# 8. count punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c838be9b-ad6d-4bfd-b9b7-9adc26f89a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['punctuation_count'] = df['Text'].apply(count_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1452b8b4-e3fa-46a0-9b7b-9eb6f4064ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new.to_csv('07_01_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62e85437-2831-4bb1-9205-59683fd6f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b6b7a8a-a28d-4408-9517-a9d0a41b95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True, padding=\"max_length\")\n",
    "    output = model(**inputs)\n",
    "    embedding = output.last_hidden_state[:, 0, :]\n",
    "    return embedding.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634969c6-2bd3-4b17-86ed-304c8ad4074c",
   "metadata": {},
   "source": [
    "# 9. generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cf61cd2-93bf-44f8-b43d-3a90b22de2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embedding'] = df['Text'].apply(get_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4944f9cb-a71b-4a02-b5af-aa70339e2724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy array\n",
    "def convert_to_numpy_array(embd_str):\n",
    "    #print(f\"string: {embd_str}\")\n",
    "    #print(type(embd_str))\n",
    "    # embd_str_clean = embd_str.replace(\"\\n\", \" \").replace(\" \", \" \")\n",
    "    #print(type(embd_str_clean))\n",
    "    #print(f\"string cleansing: {embd_str_clean}\")\n",
    "    # embd_str_clean = embd_str.strip('[]')\n",
    "    #print(type(embd_str_clean))\n",
    "    #print(f\"removing the outer brackets {embd_str_clean}\")\n",
    "    #embd_list = embd_str.split()\n",
    "    #print(f\"Split the string into individual values {embd_list}\")\n",
    "    embed_array = np.array(embd_str, dtype = float)\n",
    "    #print(f\"list to array {embed_array}\")\n",
    "    return embed_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e5834-cfeb-46b8-9012-6a2470d90691",
   "metadata": {},
   "source": [
    "# 10. renaming the column name , similler to the column name, when the model was trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc5d33df-5862-4c3f-9930-223da19697aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'token_length': 'count_tokens'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13304c9e-c589-40c6-8014-05381151404e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sentence_id', 'Text', 'Tokens', 'text_length', 'cleaned_text',\n",
       "       'cleaned_text_length', 'roberta_sentiment', 'roberta_sent_pos',\n",
       "       'roberta_sent_neg', 'roberta_sent_mixed', 'roberta_sentiment_class',\n",
       "       'count_tokens', 'counts', 'names', 'organizations', 'dates',\n",
       "       'punctuation_count', 'embedding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7920566-ad6f-4417-8e5e-e43c4c24051b",
   "metadata": {},
   "source": [
    "# 11. convert the embedding into numoy array for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e72d0d86-2bac-4ff1-a5f9-098713f54388",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['converted_embedding'] = df['embedding'].apply(convert_to_numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d34585e4-c360-4f15-9881-27e50541ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({'text_length': 'int32', 'roberta_sent_neg': 'float32', 'roberta_sent_mixed': 'float32', 'roberta_sent_pos': 'float32',\n",
    "               'names': 'int32', 'organizations': 'int32', 'dates': 'int32', 'count_tokens': 'int32', 'cleaned_text_length': 'int32', 'punctuation_count': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50ff09ff-dd8b-4526-ba2a-7a7774b158e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n"
     ]
    }
   ],
   "source": [
    "print(df['converted_embedding'].iloc[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bfd3c0-0718-4cce-b7c1-7a60a9a7c60b",
   "metadata": {},
   "source": [
    "# 12. flatten the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5d4edaf-538a-476b-b0dc-103f36bef484",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['flattened_embedding'] = df['converted_embedding'].apply(lambda x: np.squeeze(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "248274f5-0bb4-4c46-8e2c-78e34c35092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['flattened_embedding'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab99533c-9353-440d-846d-6de18d1a24ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18c33f6b-ce25-464b-8545-3cd78a3f105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('english_test_08_01.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c0e210-66df-4c4d-94b4-721fcbc4f976",
   "metadata": {},
   "source": [
    "# 13. PCA for reducing dimensionality of embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00c4ac65-f080-47d8-b76b-06253e3d904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 50)\n",
    "reduced_embeddings = pca.fit_transform(df['flattened_embedding'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e7cd484-a5a7-47e0-ae3a-4320b2d06ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings = np.array(reduced_embeddings, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28291f16-b8d6-443b-8ac8-022622b4460e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(341, 50)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fd7c2d1-2023-434a-8f8a-22e795c132b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converted_embedding = np.vstack(df['converted_embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4bba4cea-b622-440a-8fb2-249eefd19626",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_features = df[['text_length', 'roberta_sent_neg', 'roberta_sent_mixed',\n",
    "                            'roberta_sent_pos', 'names', 'organizations', 'dates',\n",
    "                            'count_tokens', 'cleaned_text_length', 'punctuation_count']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cadcaab0-dc97-48a1-9fc1-21eef58c0904",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([reduced_embeddings, additional_features]) # stack horizontally to add additional features with corresponding embedding vectors\n",
    "# y = df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce920b2f-b439-4935-9d5b-9cf7406ce6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path_lr = 'models/logistic_reggression.pkl'\n",
    "model_lr = joblib.load(model_path_lr)\n",
    "predictions_lr = model_lr.predict(X)\n",
    "predictions_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be2bd31b-8837-485c-9d88-06a4dd6b4943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path_rfc = 'models/rfc.pkl'\n",
    "model_rfc = joblib.load(model_path_rfc)\n",
    "predictions_rfc = model_rfc.predict(X)\n",
    "predictions_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c85b5d4-6b1b-42b7-9892-55337265e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_lr = pd.DataFrame({\n",
    "    'id': df['Sentence_id'],\n",
    "    'predictions': predictions_lr\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13863d3e-07dd-4932-a2d5-df395c152fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_lr['predictions'] = test_data_lr['predictions'].apply([lambda x: 'No' if x == 0 else 'Yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f9d75183-a1ba-41db-aec0-5cad72c78bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id             18\n",
      "predictions    18\n",
      "dtype: int64\n",
      "id             323\n",
      "predictions    323\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test_data_lr[test_data_lr['predictions'] == 'Yes'].count())\n",
    "print(test_data_lr[test_data_lr['predictions'] == 'No'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9054ba97-89db-42a2-9919-4bad1b1fc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_rfc = pd.DataFrame({\n",
    "    'id': df['Sentence_id'],\n",
    "    'predictions': predictions_rfc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de6c2ddf-5fa5-4f32-b36b-73397a188612",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_rfc['predictions'] = test_data_rfc['predictions'].apply([lambda x: 'No' if x == 0 else 'Yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ae281bb5-5346-4cba-a84b-2b6e931566f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id             18\n",
      "predictions    18\n",
      "dtype: int64\n",
      "id             323\n",
      "predictions    323\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test_data_rfc[test_data_rfc['predictions'] == 'Yes'].count())\n",
    "print(test_data_rfc[test_data_rfc['predictions'] == 'No'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f9e7184-e74f-4fe7-a449-b925d530891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df = pd.read_csv('data/CT24_checkworthy_english/CT24_checkworthy_english/test_english_gold/CT24_checkworthy_english_test_gold.tsv',delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "386ec1b9-c957-4e29-88df-5ef7fb0a26eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35988</td>\n",
       "      <td>They said they were just going to get inspecto...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35991</td>\n",
       "      <td>And from that point on, I've voted to -- I mov...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36029</td>\n",
       "      <td>I sit on the Senate Armed Services Committee.</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36043</td>\n",
       "      <td>We need to depend on all of our tools -- diplo...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36063</td>\n",
       "      <td>And that is -- and I don't know if my colleagu...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence_id                                               Text class_label\n",
       "0        35988  They said they were just going to get inspecto...         Yes\n",
       "1        35991  And from that point on, I've voted to -- I mov...         Yes\n",
       "2        36029      I sit on the Senate Armed Services Committee.          No\n",
       "3        36043  We need to depend on all of our tools -- diplo...          No\n",
       "4        36063  And that is -- and I don't know if my colleagu...          No"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2fabc148-7025-4e5c-82d1-49c4478073c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7360703812316716\n",
      "Precision: 0.4444444444444444\n",
      "Recall: 0.09090909090909091\n",
      "F1 Score: 0.1509433962264151\n"
     ]
    }
   ],
   "source": [
    "## for LR\n",
    "df_merged_lr = pd.merge(gold_df, test_data_lr, left_on='Sentence_id', right_on='id', how='inner')\n",
    "\n",
    "# Correct the typo: 'gold_lables' to 'gold_labels'\n",
    "gold_labels = df_merged_lr['class_label'].tolist()\n",
    "pred_labels = df_merged_lr['predictions'].tolist()\n",
    "\n",
    "# Now use 'gold_labels' consistently\n",
    "acc = accuracy_score(gold_labels, pred_labels)\n",
    "precision = precision_score(gold_labels, pred_labels, pos_label='Yes', average='binary')\n",
    "recall = recall_score(gold_labels, pred_labels, pos_label='Yes', average='binary')\n",
    "f1 = f1_score(gold_labels, pred_labels, pos_label='Yes', average='binary')\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "51963e7a-5642-4c5e-9191-f409221332bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7360703812316716\n",
      "Precision: 0.4444444444444444\n",
      "Recall: 0.09090909090909091\n",
      "F1 Score: 0.1509433962264151\n"
     ]
    }
   ],
   "source": [
    "# for rfc\n",
    "df_merged_rfc = pd.merge(gold_df, test_data_rfc, left_on='Sentence_id', right_on='id', how='inner')\n",
    "\n",
    "# Correct the typo: 'gold_lables' to 'gold_labels'\n",
    "gold_labels = df_merged_rfc['class_label'].tolist()\n",
    "pred_labels = df_merged_rfc['predictions'].tolist()\n",
    "\n",
    "# Now use 'gold_labels' consistently\n",
    "acc = accuracy_score(gold_labels, pred_labels)\n",
    "precision = precision_score(gold_labels, pred_labels, pos_label='Yes', average='binary')\n",
    "recall = recall_score(gold_labels, pred_labels, pos_label='Yes', average='binary')\n",
    "f1 = f1_score(gold_labels, pred_labels, pos_label='Yes', average='binary')\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496dde86-95b9-43c2-9a4c-89573e99b89c",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
